{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Legal Issues:\n",
    "        Don't worry about it.\n",
    "        But keep note, I should respect robots.txt\n",
    "        But if I do collect the data, DO NOT give it to others.\n",
    "        Just don't sell or advertise that the data is mine.\n",
    "        Don't mess with financial or gov data.\n",
    "        Don't infringe on actual copyright stuff.\n",
    "        Don't enter anything that requires permission, passwords.\n",
    "        Don't scrape emails, usernames.\n",
    "        Don't spam forms.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    Dealing With HTTP Errors:\n",
    "        Don't move scrapers too quickly.\n",
    "        Change headers.\n",
    "        Don't anything a human wouldn't.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    Scraping Remotely:\n",
    "        Use TOR browser (bounces IP address)\n",
    "        DuckDuckGo does not store cookies\n",
    "        Scrape on Google Cloud. Has access to changing IP addresses.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "           'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Go to starting page.\n",
    "    Gather all the links in that one page.\n",
    "    Go to next page.\n",
    "    Repeat.\n",
    "    Stop when the page does not exist. \n",
    "        Problem: the page exists even when it shouldn't\n",
    "        Example: searching &start=1000 seems to produce the first page\n",
    "        Solution: need to create a numpy array of links,\n",
    "            append the new links if the new links are not found\n",
    "            stop if the links are found\n",
    "        Current Solution (going to change this): \n",
    "            Specify a limit\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.9090261459350586 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gather_Yelp_data(url, cap_num):\n",
    "    \"\"\"\n",
    "        Given a starting url and a page limit,\n",
    "        gather all the links to businesses.\n",
    "        Go to the next page.\n",
    "        Gather all links.\n",
    "        Repeat a specified number of times.\n",
    "        Return a numpy array of all links gathered.\n",
    "    \"\"\"\n",
    "    item_number = 0\n",
    "    websites = np.array([])\n",
    "    while item_number <= cap_num:\n",
    "        try:\n",
    "            #html = urlopen(url + \"&start=\" + str(item_number*10))\n",
    "            #bs = BeautifulSoup(html.read(), 'lxml')\n",
    "            req = session.get(url + \"&start=\" + str(item_number*10), headers=headers)\n",
    "            bs = BeautifulSoup(req.text, 'lxml')\n",
    "            links = bs.findAll('a', {'class': \"biz-name js-analytics-click\"})\n",
    "            for link in links:\n",
    "                #print (link)\n",
    "                link_string = link.attrs['href']\n",
    "                websites = np.append(websites, np.array([link_string]))\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep (random.random())\n",
    "        item_number += 1\n",
    "    return websites\n",
    "starturl = \"\"\"https://www.yelp.com/search?find_desc=cafe&find_loc=60637\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "sites = gather_Yelp_data(starturl, 0)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 21.07632803916931 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Link, Num]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Huge Problem of Inconsistency:\n",
    "    Sometimes things are returned, others nothing is returned. \n",
    "    Why? I don't know. Running the same thing returns different results.\n",
    "    So how do I know when I'm not getting any result? Label Them\n",
    "\"\"\"\n",
    "def gather_Yelp_data(url, cap_num):\n",
    "    item_number = 0\n",
    "    websites = []\n",
    "    num_label = []\n",
    "    while item_number <= cap_num:\n",
    "        try:\n",
    "            req = session.get(url + \"&start=\" + str(item_number*10), headers=headers)\n",
    "            bs = BeautifulSoup(req.text, 'lxml')\n",
    "            links = bs.findAll('a', {'class': \"biz-name js-analytics-click\"})\n",
    "            for link in links:\n",
    "                link_string = link.attrs['href']\n",
    "                websites.append (link_string)\n",
    "                num_label.append (item_number)\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep (random.random())\n",
    "        item_number += 1\n",
    "    return pd.DataFrame ({'Link':websites, 'Num': num_label}), cap_num\n",
    "starturl = \"\"\"https://www.yelp.com/search?find_desc=cafe&find_loc=60637\"\"\"\n",
    "\n",
    "yelp_page_num = 0\n",
    "start_time = time.time()\n",
    "sites, yelp_page_num = gather_Yelp_data(starturl, yelp_page_num)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = session.get(starturl + \"&start=\" + str(0), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/adredir?ad_business_id=JbAkhS96tW5lbOLjVwsQlQ&campaign_id=v9vTEM8xECDPpPldqe7BxQ&click_origin=search_results&placement=above_search&redirect_url=https%3A%2F%2Fwww.yelp.com%2Fbiz%2Fedible-arrangements-chicago-11&request_id=6b8306fcab115a64&signature=93ab4e2d026292b1e023ae6e9feff22376ad737cbc631f8d8da30a0c65f79a3c&slot=0\n",
      "/adredir?ad_business_id=jIUfYtTGz3nEXxGid5kH3w&campaign_id=eBRciaMya6NTx6XfPYxGlw&click_origin=search_results&placement=above_search&redirect_url=https%3A%2F%2Fwww.yelp.com%2Fbiz%2Fritas-italian-ice-burbank-2&request_id=6b8306fcab115a64&signature=6fc20c1c0aaa99b8bb589851a40cf41480126a22267621943ffb027c7907be52&slot=1\n",
      "/biz/plein-air-cafe-and-eatery-chicago-2?osq=cafe\n",
      "/biz/robust-coffee-lounge-chicago?osq=cafe\n",
      "/biz/harper-cafe-chicago?osq=cafe\n",
      "/biz/build-coffee-chicago?osq=cafe\n",
      "/biz/sanctuary-cafe-chicago?osq=cafe\n",
      "/biz/greenline-coffee-chicago?osq=cafe\n",
      "/biz/grounds-of-being-the-divinity-school-coffee-shop-chicago?osq=cafe\n",
      "/biz/caf%C3%A9-logan-chicago?osq=cafe\n",
      "/biz/cafe-53-chicago?osq=cafe\n",
      "/biz/dollop-coffee-chicago-5?osq=cafe\n",
      "/adredir?ad_business_id=WE6msctLR1hQqfqclioI-A&campaign_id=jahk60S4LWQC2EURAye4pg&click_origin=search_results&placement=below_search&redirect_url=https%3A%2F%2Fwww.yelp.com%2Fbiz%2Fmcdonalds-chicago-97&request_id=6b8306fcab115a64&signature=0750e951f1e0008d71e884479b730d896ecd066234742026b031e9c4862b8eec&slot=0\n"
     ]
    }
   ],
   "source": [
    "bs = BeautifulSoup(req.text, 'lxml')\n",
    "links = bs.findAll('a', {'class': \"biz-name js-analytics-click\"})\n",
    "for link in links:\n",
    "    link_string = link.attrs['href']\n",
    "    print (link_string)\n",
    "websites = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Going to have to search by zip code.\n",
    "    There are 42000 zip codes in US.\n",
    "    How do we find the number of cafes in Yelp?\n",
    "    Take a random sample (say 30) of zip codes.\n",
    "    Search Yelp through the Zip Code.\n",
    "    Find average number of businesses. \n",
    "    Multiply that by 42000\n",
    "'''\n",
    "\n",
    "'''\n",
    "import random\n",
    "for x in range(30):\n",
    "  print (random.randint(10000,99999)) \n",
    "'''\n",
    "\n",
    "num_cafes = [\n",
    "    21, 10, 69, 33, 21, 55, 18, 39, 16, 47, 16, 24, 16, 37, 28, 26, 8, 37, 22, 5, 81\n",
    "]\n",
    "average_num_Cafe = sum(num_cafes) / len (num_cafes) # 29.952380952380953\n",
    "num_cafe_in_US = average_num_Cafe * 43000 # 1287952\n",
    "\n",
    "'''\n",
    "    About 1.945 seconds per visited webpage (10 business for one webpage)\n",
    "    Then about 69.6 hours for visiting all webpages and collecting websites.\n",
    "    \n",
    "    About 7.5 seconds per taking all review pages of a single business.\n",
    "    About 3.3 seconds to take all additional info.\n",
    "    10.8 seconds total.\n",
    "    So then 161 days.\n",
    "    \n",
    "'''\n",
    "print (num_cafe_in_US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Scraping an individual business:\n",
    "    Information that I want:\n",
    "        Address\n",
    "        Pricing\n",
    "        Health Score\n",
    "        Extra info on the right\n",
    "        Reviews\n",
    "\"\"\"\n",
    "def getBasicInfo(url):\n",
    "    \"\"\"\n",
    "        Given url of one business,\n",
    "        gather all basic info except the reviews.\n",
    "        Returns a dictionary.\n",
    "    \"\"\"\n",
    "    dict_of_info = {}\n",
    "    html = urlopen(url)\n",
    "    bs = BeautifulSoup(html.read(), 'lxml')\n",
    "    dict_of_info['Address'] = bs.find_all('address')[-1].text\n",
    "    price_and_health = bs.find_all('dd', {'class': 'nowrap'})\n",
    "    dict_of_info['Price Range'] = price_and_health[0].text\n",
    "    try:\n",
    "        dict_of_info['Health Score'] = price_and_health[1].text\n",
    "    except:\n",
    "        pass\n",
    "    more_biz_info = bs.find('div', {'class': \"short-def-list\"}).find_all('dl',)\n",
    "    for info in more_biz_info:\n",
    "        dict_of_info[info.find('dt').text] = info.find('dd').text\n",
    "    return dict_of_info\n",
    "\n",
    "start_time = time.time()\n",
    "greenline_coffee = \"\"\"https://www.yelp.com/biz/greenline-coffee-chicago\"\"\"\n",
    "greenline = getBasicInfo(greenline_coffee)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_white_space(i_dict):\n",
    "    for key in i_dict.keys():\n",
    "        i_dict[key.strip()] = i_dict.pop(key)\n",
    "    for key in i_dict.keys():\n",
    "        i_dict[key] = i_dict[key].strip()\n",
    "    return i_dict\n",
    "\n",
    "greenline = clear_white_space(clear_white_space (greenline)) # clear it twice\n",
    "greenline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Now to get the reviews:\n",
    "    Want:\n",
    "        star number\n",
    "        date\n",
    "        content\n",
    "\"\"\"\n",
    "def get_reviews_in_single_page(bs):\n",
    "    \"\"\"\n",
    "        Given a parsed url,\n",
    "        get all review info in a single page.\n",
    "        Returns a df.\n",
    "    \"\"\"\n",
    "    review_dict = {}\n",
    "    # this gets the box of info of Yelp review\n",
    "    review_form = bs.find(\n",
    "        'ul', {'class': \"ylist ylist-bordered reviews\"}\n",
    "        ).find_all(\n",
    "        'div', {'class': \"review-wrapper\"})\n",
    "\n",
    "    rating_list = []\n",
    "    date_list = []\n",
    "    comment_list = []\n",
    "    for review in review_form[1:]:\n",
    "        # gets number of stars\n",
    "        rating_list.append(\n",
    "            review.find('div', {'class': re.compile('i-stars i-stars--regular-.*')}).img.attrs['alt'])\n",
    "        # gets dates\n",
    "        date_list.append(\n",
    "            review.find('span', {'class': \"rating-qualifier\"}).text)\n",
    "        # gets comments\n",
    "        comment_list.append(\n",
    "            review.find('p', {'lang': \"en\"}).text)\n",
    "    review_dict['Star Ratings'] = rating_list\n",
    "    review_dict['Date'] = date_list\n",
    "    review_dict['Comment'] = comment_list   \n",
    "    return pd.DataFrame(review_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_reviews(url, page_limit):\n",
    "    page_num = 0\n",
    "    review_df = pd.DataFrame()\n",
    "    review_url = url\n",
    "    while page_num <= page_limit:\n",
    "        html = urlopen(review_url)\n",
    "        bs = BeautifulSoup(html.read(), 'lxml')\n",
    "        review_df = review_df.append(get_reviews_in_single_page(bs))\n",
    "        page_num += 1\n",
    "        review_url = url + '?start=' + str(page_num * 20)  # 20 reviews in each page\n",
    "    review_df = review_df.drop_duplicates(['Comment'])  # .reset_index().drop('index',axis=1)\n",
    "    return review_df\n",
    "\n",
    "start_time = time.time()\n",
    "greenline_coffee_reviews = get_all_reviews(greenline_coffee, 2)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "greenline_coffee_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test code on a different business\n",
    "sanc_url = \"\"\"https://www.yelp.com/biz/sanctuary-cafe-chicago?osq=cafe\"\"\"\n",
    "sanctuary_cafe = getBasicInfo(sanc_url)\n",
    "sanctuary_cafe = clear_white_space(clear_white_space(sanctuary_cafe))\n",
    "sanctuary_cafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sanctuary_cafe_reviews = get_all_reviews(sanc_url, 2)\n",
    "sanctuary_cafe_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "greenline_coffee_reviews.to_csv('Greenline Coffee Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
